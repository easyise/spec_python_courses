{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7615cb2c",
   "metadata": {},
   "source": [
    "# Python и машинное обучение: нейронные сети и компьютерное зрение\n",
    "\n",
    "# Модуль 3a: Полносвязная нейросеть и классификация изображений\n",
    "\n",
    "\n",
    "- Обучение на мини-пакетах (мини-батчах) данных: класс ```Dataset```\n",
    "- Использование многослойного персептрона на наборе данных MNIST (50K рукописных цифр ч/б 28х28 пикс)\n",
    "- Сохранение и загрузка моделей\n",
    "- Регуляризация модели: добавление ```dropout```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753094a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91672fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from deeplearn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \\\n",
    "    \"mps\" if torch.backends.mps.is_built() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(X.shape)\n",
    "print(X.max(), X.min())\n",
    "print(y[:12])\n",
    "print(np.unique(y))\n",
    "print(np.unique(y).shape)\n",
    "print(X[0].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e8ba5",
   "metadata": {},
   "source": [
    "### Загрузка данных в модель в виде мини-батчей\n",
    "\n",
    "Выше мы обучали модель на всех имеющихся данных. Но при обучении нейросетей данные чаще всего загружают в виде мини-пакетов (мини-батчей). Это делается для того, чтобы:\n",
    "- обучать модель на разнообразии данных: в рамках одной эпохи каждый цикл обучения она будет видеть уникальные данные;\n",
    "- минимизировать объем памяти, используемой при обучении;\n",
    "- ускорить работу оптимизатора: он будет гораздо быстрее обрабатывать небольшой объем данных6 нежели чем весь датасет.\n",
    "\n",
    "Для этого можно использовать встроенный в PyTorch механизм генерации мини-батчей. На базе стандартного генератора PyTorch можно создать свой собственный, который будет отправлять в модель именно ваши данные в нужном именно вам виде.\n",
    "\n",
    "Для начала нужно создать свой класс на базе класса PyTorch ```Dataset```:\n",
    "\n",
    "```python\n",
    "class Dataset(object):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "````\n",
    "\n",
    "Затем на базе этого класса будут созданы объекты-генераторы данных, уже на этапе создания в них будет передана функция ```transforms.ToTensor()``` для преобразования изображений \"на лету\", по требованию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617aca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс генератора:\n",
    "class DatasetDigits(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # сразу представляем цифру как ndarray размерностью (Height * Width * Channels)\n",
    "        # конвертируем цифры в np.uint8 [Unsigned integer (0 to 255)] - стандарт для изображений\n",
    "        # чтобы отрабатывала стандартная функция ToTensor(), мы определяем размерность тензора (H, W, C)\n",
    "        image = self.X[index].astype(np.uint8).reshape((8, 8, 1))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.y is not None:\n",
    "            return (image, self.y[index])  \n",
    "        else:\n",
    "            return image\n",
    "\n",
    "# снова сделаем разбиение на обучающую и валидационную выборки\n",
    "# старые переменные X_train, X_val... - это нормированные тензоры, а нам нужны изображения в исходном формате\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=20231110,\n",
    "                                                   stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# создадим генераторы обучающих и тестовых данных:\n",
    "train_data = DatasetDigits(X_train, y_train, transform=transforms.ToTensor())\n",
    "val_data = DatasetDigits(X_val, y_val, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_generator = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(list(range(y_train.shape[0]))))\n",
    "val_generator=  torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(list(range(y_val.shape[0]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batches(model, \n",
    "                  train_generator,\n",
    "                  valid_generator,\n",
    "                  batch_size=20, epochs=40, report_positions=20, **kwargs):\n",
    "    \n",
    "    results = {'epoch_count': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    # прогоняем данные по нейросети\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = valid_loss = 0.0; \n",
    "        train_correct = valid_correct = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_generator:\n",
    "            \n",
    "            X_batch = X_batch.to(device); y_batch = y_batch.to(device)\n",
    "            \n",
    "            y_logps = model(X_batch) #логарифмы вероятности отнесения к классам\n",
    "            loss = criterion(y_logps, y_batch) #кросс-энтропия\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.data.item()\n",
    "            train_correct += accuracy_fn(y_logps, y_batch) * y_batch.shape[0]\n",
    "            \n",
    "        train_loss /= len(train_generator.dataset)\n",
    "        train_acc = 100 * train_correct / len(train_generator.dataset)\n",
    "\n",
    "        # Валидацию тоже делаем по батчам\n",
    "        model.eval()         \n",
    "        \n",
    "        for valid_batches, (X_val_batch, y_val_batch) in enumerate(valid_generator):\n",
    "            X_val_batch = X_val_batch.to(device); y_val_batch = y_val_batch.to(device)\n",
    "            y_batch_logps = model(X_val_batch)\n",
    "            loss = criterion(y_batch_logps, y_val_batch)\n",
    "            \n",
    "            valid_loss += loss.data.item()\n",
    "            valid_correct += accuracy_fn(y_batch_logps, y_val_batch) * y_val_batch.shape[0]\n",
    "            \n",
    "        valid_loss /= len(valid_generator.dataset)\n",
    "        valid_acc = 100 * valid_correct / len(valid_generator.dataset)\n",
    "        \n",
    "        results['epoch_count'] += [epoch]\n",
    "        results['train_loss'] += [ train_loss ]\n",
    "        results['train_acc'] += [ train_acc ]\n",
    "        results['val_loss'] += [ valid_loss ]\n",
    "        results['val_acc'] += [ valid_acc ]\n",
    "        \n",
    "        if epoch % (epochs // report_positions) == 0 or epochs<50:\n",
    "            print(f\"Epoch: {epoch+1:4.0f} | Train Loss: {train_loss:.5f}, \"+\\\n",
    "                  f\"Accuracy: {train_acc:.2f}% | \\\n",
    "                Validation Loss: {valid_loss:.5f}, Accuracy: {valid_acc:.2f}%\")\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09c01d",
   "metadata": {},
   "source": [
    "Будем обучать ту же модель, что и в прошлый раз, ```MLPDigits_vary()```, только добавим ей \"выпрямляющий\" код на входе, чтобы она могла работать с тензорами-изображениями PyTorch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0faa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDigits_vary(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation='sigmoid', hidden=52, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8*8, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 8 * 8) # изображение приходит в формате (1,8,8), делаем его плоским\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481a2b4",
   "metadata": {},
   "source": [
    "Поэкспериментируйте с моделью, варьируйте количество эпох и размер пакета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8369c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "vary = 'batch_size' #здесь пишем, что варьируем\n",
    "var_values = [64, 25, 10] # здесь перечисляем варианты\n",
    "\n",
    "\n",
    "dict_vary = {'hidden': 12,\n",
    "            'activation': 'tanh',\n",
    "            'batch_size': 20,\n",
    "            'lr': 0.002,\n",
    "            'momentum': 0.9,\n",
    "            'optimizer': 'RMSprop',\n",
    "            'epochs': 30}\n",
    "\n",
    "dict_acc = {} # here we collect data for comparison\n",
    "\n",
    "for var in var_values:\n",
    "    print(f\"{vary}: {var}\")\n",
    "    dict_vary[vary] = var\n",
    "    model = MLPDigits_vary( **dict_vary ).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum']) \\\n",
    "            if dict_vary['optimizer']=='RMSprop' else \\\n",
    "                torch.optim.Adam(model.parameters(), lr=dict_vary['lr']) \\\n",
    "            if dict_vary['optimizer']=='Adam' else \\\n",
    "                    torch.optim.SGD(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum'])\n",
    "    \n",
    "    # добавляем создание генератора для нужного нам количества батчей\n",
    "    train_generator = torch.utils.data.DataLoader(train_data, \n",
    "                                                  batch_size=dict_vary['batch_size'],\n",
    "        sampler=SubsetRandomSampler(list(range(y_train.shape[0]))))\n",
    "    valid_generator = torch.utils.data.DataLoader(val_data, \n",
    "                                                  batch_size=dict_vary['batch_size'],\n",
    "        sampler=SubsetRandomSampler(list(range(y_val.shape[0]))))\n",
    "    \n",
    "    results = train_batches(model, train_generator, valid_generator, report_positions=10, **dict_vary)\n",
    "    dict_acc[f'err_{var}'] = results['val_loss']\n",
    "    dict_acc[f'acc_{var}'] = results['val_acc']\n",
    "    dict_acc[f'epochs_{var}'] = results['epoch_count']\n",
    "    \n",
    "    plot_results(results)\n",
    "\n",
    "    \n",
    "fig_var, axs_var = plt.subplots(1,2)\n",
    "fig_var.set_size_inches(10,3)\n",
    "for var in var_values:\n",
    "    axs_var[0].plot(dict_acc[f'epochs_{var}'], dict_acc[f'err_{var}'], label=f'loss on {vary}={var}')\n",
    "    axs_var[0].legend()\n",
    "    axs_var[1].plot(dict_acc[f'epochs_{var}'], dict_acc[f'acc_{var}'], label=f'acc on {vary}={var}')\n",
    "    axs_var[1].legend()\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "summary(model, \n",
    "        input_size=X_train.shape, \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        device=device\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53bd9d9",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "Попробуем теперь обучить нашу модель на датасете MNIST, это те же \"рукописные цифры\", но в разрешении 28x28 и в количестве 50000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=transforms.ToTensor())\n",
    "valset = datasets.MNIST('./data', download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5031cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# параметры нормализации\n",
    "imgs = torch.stack([img for img, _ in trainset], dim=0)\n",
    "\n",
    "mean = imgs.view(1, -1).mean(dim=1)    # or imgs.mean()\n",
    "std = imgs.view(1, -1).std(dim=1)     # or imgs.std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e400df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "trainset = datasets.MNIST('./data', download=True, train=True, transform=mnist_transforms)\n",
    "valset = datasets.MNIST('./data', download=True, train=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "train_generator_MNIST = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "val_generator_MNIST = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df03b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# визуализируем батч\n",
    "dataiter = iter(train_generator_MNIST)\n",
    "images, labels = next(dataiter)\n",
    "images = images.numpy()\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray_r')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdcde6",
   "metadata": {},
   "source": [
    "Cоздадим модель на базе последней, повысим размерность входного слоя до 28x28:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MNIST_vary(nn.Module):\n",
    "    \n",
    "    def __init__(self, activation='sigmoid', hidden=52, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # изображение приходит в формате (1,8,8), делаем его плоским\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0086d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_vary = {'hidden': 532,\n",
    "            'activation': 'tanh',\n",
    "            'batch_size': batch_size,\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.9,\n",
    "            'optimizer': 'RMSprop',\n",
    "            'epochs': 10}\n",
    "\n",
    "model = MLP_MNIST_vary( **dict_vary ).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9) \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "results = train_batches(model, \n",
    "                        train_generator_MNIST, \n",
    "                        val_generator_MNIST, report_positions=10, **dict_vary)\n",
    "\n",
    "plot_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836245f5",
   "metadata": {},
   "source": [
    "**ЗАДАНИЕ**\n",
    "\n",
    "Варьируйте различные параметры модели, старайтесь получить лучший результат (max точности на валидационной выборке за минимум эпох).\n",
    "\n",
    "Сохраните лучшую модель с помощью \n",
    "```python\n",
    "torch.save(model, 'MLP_MNIST.pt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4870dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vary = 'batch_size' #здесь пишем, что варьируем\n",
    "var_values = [256, 1024] # здесь перечисляем варианты\n",
    "\n",
    "\n",
    "dict_vary = {'hidden': 532,\n",
    "            'activation': 'tanh',\n",
    "            'batch_size': batch_size,\n",
    "            'lr': 0.01,\n",
    "            'momentum': 0.9,\n",
    "            'optimizer': 'SGD',\n",
    "            'epochs': 5}\n",
    "\n",
    "dict_acc = {} # here we collect data for comparison\n",
    "\n",
    "for var in var_values:\n",
    "    print(f\"{vary}: {var}\")\n",
    "    dict_vary[vary] = var\n",
    "    model = MLP_MNIST_vary( **dict_vary ).to(device)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum']) \\\n",
    "            if dict_vary['optimizer']=='RMSprop' else \\\n",
    "                torch.optim.Adam(model.parameters(), lr=dict_vary['lr']) \\\n",
    "            if dict_vary['optimizer']=='Adam' else \\\n",
    "                    torch.optim.SGD(model.parameters(), lr=dict_vary['lr'], momentum=dict_vary['momentum'])\n",
    "\n",
    "    train_generator_MNIST = torch.utils.data.DataLoader(trainset, batch_size=dict_vary['batch_size'], shuffle=True)\n",
    "    val_generator_MNIST = torch.utils.data.DataLoader(valset, batch_size=dict_vary['batch_size'], shuffle=True)\n",
    "\n",
    "    results = train_batches(model, train_generator_MNIST,\n",
    "                        val_generator_MNIST, report_positions=min(10, dict_vary['epochs']), **dict_vary)\n",
    "    dict_acc[f'err_{var}'] = results['val_loss']\n",
    "    dict_acc[f'acc_{var}'] = results['val_acc']\n",
    "    dict_acc[f'epochs_{var}'] = results['epoch_count']\n",
    "\n",
    "    plot_results(results)\n",
    "\n",
    "\n",
    "fig_var, axs_var = plt.subplots(1,2)\n",
    "fig_var.set_size_inches(10,3)\n",
    "for var in var_values:\n",
    "    axs_var[0].plot(dict_acc[f'epochs_{var}'], dict_acc[f'err_{var}'], label=f'loss on {vary}={var}')\n",
    "    axs_var[0].legend()\n",
    "    axs_var[1].plot(dict_acc[f'epochs_{var}'], dict_acc[f'acc_{var}'], label=f'acc on {vary}={var}')\n",
    "    axs_var[1].legend()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "summary(model,\n",
    "        input_size=X_train.shape,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
    "        device=device\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f67e6b",
   "metadata": {},
   "source": [
    "## Регуляризация добавлением слоя Dropout\n",
    "\n",
    "Добавим в модель \"прореживание\" - слой dropout, который в момент обучения блокирует ряд нейронов (позиций в матрицах соотв. слоев)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_MNIST_dropout_vary(nn.Module):\n",
    "\n",
    "    def __init__(self, activation='sigmoid', hidden=52, dropout=0.25, **kwargs):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, 10)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = eval(f'F.{activation}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28) # изображение приходит в формате (1,8,8), делаем его плоским\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22543263",
   "metadata": {},
   "source": [
    "**ЗАДАНИЕ**\n",
    "\n",
    "Возьмите у лучшей модели из предыдущего задания ее параметры, создайте новую модель на базе класса ```MLP_MINST_dropout_vary```. Варьруйте значение дропаута, посмотрите на качество работы той или иной модели, сделайте выводы.\n",
    "\n",
    "Сохраните лучшую модель с помощью \n",
    "```python\n",
    "torch.save(model, 'MLP_MNIST_dropout.pt')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9ee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код здесь\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
