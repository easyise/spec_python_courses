{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Курс \"Программирование на языке Python. Уровень 4. Анализ и визуализация данных на языке Python. Библиотеки numpy, pandas, matplotlib\"\n",
    "\n",
    "# Модуль 5. Библиотека pandas. Работа с датасетами.\n",
    "\n",
    "- Загрузка датасетов\n",
    "- Обработка отсутствующих данных\n",
    "- Поиск и удаление дублей\n",
    "- Создание новых признаков, функции ```apply()``` и ```applymap()```\n",
    "- Категориальные признаки, функция ```cut()```, dummy-признаки\n",
    "- Горизонтальные и вертикальные объединения, функции ```merge()``` и ```concat()```\n",
    "- \"Широкий\" и \"Длинный\" форматы таблиц (stack/unstack)\n",
    "- Сохранение датасетов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузите необходимые библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка датасетов\n",
    "\n",
    "Pandas поддерживает загрузку данных из множества источников. Чаще всего придется работать с данными в форматах CSV, XLSX и JSON, а также загружать их из базы данных.\n",
    "\n",
    "Рассмотрим загрузку данных из файла формата csv - данных, разделенных запятыми. Посмотрим содержимое файла, который мы будем загружать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/load_example1.csv') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для загрузки будем использовать функцию ```pd.read_csv()```. Укажите в качестве параметра имя файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, как ведет себя функция по умолчанию:\n",
    " - названия колонок соответствуют содержимому первой строки файла\n",
    " - индекс по умолчанию - последовательность чисел.\n",
    " \n",
    "Чтобы ```read_csv()``` включила первую строку в наш DataFrame, передайте ей параметр ```header=None```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example1.csv', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можно задать названия столбцов самостоятельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example1.csv', names=['aa', 'bb', 'cc', 'dd', 'mmessage'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы указать, что один из столбцов - индекс, используйте параметр index_col, там можно указать либо название поля, либо его порядковый номер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example1.csv', index_col='message')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# или \n",
    "df = pd.read_csv('data/load_example1.csv', index_col=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы пропустить те или иные строки, используйте параметр ```skiprows```, ему можно передать список строк, которые надо пропустить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example1.csv', skiprows=[0,1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание: указанные строки вообще не участвуют в разборе файла!\n",
    "\n",
    "При разборе CSV-файлов также могут встретиться следующие трудности:\n",
    " - вместо отсутствующих данных могут быть строки типа \"NULL\", \"n/a\" и т.п.\n",
    " - разделителями могут быть символы \";\" (особенно при выгрузке данных из русской версии Microsoft Excel), или же символ табуляции.\n",
    " \n",
    "Со всем этим может справиться функция ```read_csv()```. Загрузим файл ```data/load_example2.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/load_example2.csv') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Для указания символа \";\" в качестве разделителя, передайте фукнции параметр ```sep=';'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example2.csv', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы обработать строки \"данные отсутствуют\" в данном примере, функции ```read_csv()``` нужно передать параметр ```na_values='данные отсутствуют'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example2.csv', sep=';', na_values='данные отсутствуют')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приведение данных к нужному типу\n",
    "\n",
    "Бывает так, что данные, которые мы загружаем, содержат текстовую информацию в тех полях, где должны быть числа, даты и т.д. ```pandas``` самостоятельно разпознает и устанавливает типы данных. Посмотреть, какие типы данных ```pandas``` установил для каждой колонки, можно через свойство ```df.dtypes```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорректировать такие данные можно на уровне ```pd.Series```, используя функцию ```pd.Series.apply()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col3'].apply( type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col3.apply( str.isdecimal )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ ~df.col3.apply( str.isdecimal ), 'col3' ] = '15'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col3 = df.col3.astype(np.int64)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка отсутствующих данных\n",
    "\n",
    "С отсутствующими данными в объекте Series можно сдедать следующее:\n",
    " - удалить функцией ```.dropna()```\n",
    " - заполнить подходящим значением, используя функцию ```.fillna()```.\n",
    " \n",
    "Для поиска пустых значений используем функцию ```.isnull()```.\n",
    " \n",
    "Посмотрим, как это работает на примере первого сета. Снова загрузим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/load_example1.csv', index_col='message')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить series из позиций в 'b', содержащих NaN, можно используя булеву маску по колонке \"b\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['b'][df['b'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как работает ```.dropna()``` в Series, получим колонку 'b' в виде этого объекта:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['b'].copy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызовем ```dropna()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbd = b.dropna()\n",
    "bbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним отсутствующие значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно заполнить конкретным значением\n",
    "bbf = b.fillna(0)\n",
    "bbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а можно средним по всей Series\n",
    "bbf = b.fillna(b.mean())\n",
    "bbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае с DataFrame это работает похожим образом, только функция удаляет строки, в которых встречается хотя бы одно незаполненное значение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этой функции можно задать порог срабатывания, в зависимости от количества __заполненных подряд значений__ в строке. Например, нам нужно удалить только те строки, в которых заполнены как минимум первые три значения подряд:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на строку \"bar\" - несмотря на незаполненную ячейку, она не попала под удаление!\n",
    "\n",
    "Также можно заполнять отсутствующие данные числами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(100500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция работает и для заполнения \"пробелов\" горизонтальными/вертикальными агрегатными вычислениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы эти функции отработали внутри самого объекта и не возвращали его копию, используйте параметр ```inplace=True```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЗАДАНИЕ__ Замените отсутствующие значения в колонке b на среднее по ней, c - на 0, d - на среднее по всей матрице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск и удаление дублей\n",
    "\n",
    "Проверить, является ли уникальным индекс, можно, опросив свойство индекса ```is_unique```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получить булеву маску для дубликатов по индексу можно, вызвав метод ```.duplicated()```. Применение отрицания этой маски вернет DataFrame без строки с дублированным индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.index.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тем же методом объекта DataFrame или Series можно получить булеву маску для дубликатов записей в датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методу можно передать параметр ```keep=```, который не будет отмечать признаком True либо первый дубликат (значение first), либо последний (значение last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод можно вызвать, передав ему список признаков, в которм нужно ограничиться поиском дубликатов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалить дубликаты можно функцией ```drop_duplicates()```. Она работает так же, как и ```duplicated()```, но она возвращает новый DataFrame без дубликатов. Ее можно вызвать с параметром inplace()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[5, 'b'] = 10\n",
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание новых признаков, функции apply() и applymap()\n",
    "\n",
    "С созданием новых признаков на базе существующих данных мы уже знакомы, но часто бывает так, что для вычисления новых признаков нужно применить более сложные процедуры, чем стандартные. Для этого существуют функции ```apply()``` и ```applymap()```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(4, 3), columns=list('bde'),\n",
    "                     index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как работает метод ```apply()```. Функция, которая указана в качестве параметра этого метода принимает на вход объект Series - столбец и возвращает значение, которое объединяется в объект Series, структцрно соответствующий строке текущего DataFrame. Для вычисления по строкам и формирования столбцов функции ```apply()``` нужно передать параметр ```axis=1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функции f и ff эквивалентны:\n",
    "def f(x):\n",
    "    print(x)\n",
    "    return x.max() - x.min()\n",
    "\n",
    "ff = lambda x: x.max() - x.min()\n",
    "\n",
    "df.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавление нового вычисленного признака теперь будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff'] = df.apply(f, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от ```apply()```, ```applymap()``` вычисляется для каждого элемента и возвращает значение, которое должно быть установлено на его место."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_ = lambda x: '%.2f' % x\n",
    "df.applymap(format_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы проделать такую операцию над Series, воспользуйтесь функцией map():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['e'] = df['e'].map(format_)\n",
    "df\n",
    "df['e'] = df['e'].map(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЗАДАНИЕ__ В текущий DataFrame ```df``` добавьте строку с суммами значений 1000, если значение больше нуля, и 0 в противном случае."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЗАДАНИЕ__  В датасете Titanic проверьте признак \"Возраст\"(\"Age\") на выбросы (отрицательный возраст, посмотрите максимальный возраст - он правдоподобен?).\n",
    "Если там есть отсутствующие значения - на их место поставьте медианный возраст пассажиров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузите Titanic\n",
    "df_titanic = pd.read_csv('data/titanic.csv',\n",
    "                  index_col='PassengerId')\n",
    "\n",
    "# ваш код здесь\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Категориальные признаки, функция cut(), dummy-признаки\n",
    "\n",
    "Часто возникает задача сделать более точным один из признаков, сократив по нему количество возможных вариантов, а то и вообще сведя к одному или нескольким булевам признакам (dummy-признакам).\n",
    "\n",
    "Это может быть применено к различным количественным характеристикам (например, возраст, вес - \"несовершеннолетний\"/\"толстый\"), к географическим признакам (\"Москва\"/\"не Москва\"), к временным признакам (\"До Революции/После Революции\") и т.д.\n",
    "\n",
    "Рассмотрим создание категориальных признаков на примере работы с датасетом \"Титаник\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим признак \"Возрастная категория\"**\n",
    "\n",
    "Создавать будем двумя способами: \n",
    "1. с помощью функции, которая возвращает 1, если до 30-ти, 2, если от 30-ти до 55-ти и 3, если старше 55.\n",
    "2. с помощью функции ```pd.cut()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_category(age):\n",
    "    '''\n",
    "    < 30 -> 1\n",
    "    >= 30, <55 -> 2\n",
    "    >= 55 -> 3\n",
    "    '''\n",
    "    if age < 30:\n",
    "        return 1\n",
    "    elif age < 55:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "df_titanic['Age_category'] = df_titanic['Age'].apply(age_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь функцией ```cut()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим \"козрзинки\", в которые будем раскладывать наши категории\n",
    "bins = [0,30,55,100]\n",
    "age_categories = pd.cut(df_titanic['Age'], bins, right=False) # right=False - означает, что правая граница НЕ включена\n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы добавить требуемые метки, передадим их в виде списка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,2,3]\n",
    "age_categories = pd.cut(df_titanic['Age'], bins, labels=labels, right=False) \n",
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим их в наш датасет и сравним с тем, что мы сделали с помощью функции ```apply()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic['Age_category_1'] = pd.cut(df_titanic['Age'], bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.T.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще раз посмотрим на добавленные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic[ ['Age_category', 'Age_category_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic['Age_category_1'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тот признак, который мы создали из функции ```cut()``` стал категориальным - его значения могут принимать три величины: 1, 2 или 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление dummy-признаков\n",
    "\n",
    "В данном случае мы вместо одного признака \"возрастная категория\" с тремя возможными значениями сделаем три булевых признака. В задачах машинного обучения бывает необходимость оценить степень влияния принадлежности к той или иной группе на решение задачи, и если влияние незначительное - избавиться от такого признака. Потом, ряд алгоритмов принимает на вход только цифровые значения, и такое действие позволяет избавиться от one-hot encoding для таких признаков.\n",
    "\n",
    "Добавить их можно очень просто: функцией ```pd.get_dummies()```. При этом признак, из которого мы получаем эти dummy-признаки, не обязательно должен быть категориальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dummies = pd.get_dummies(df_titanic['Age_category'])\n",
    "age_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, названия колонок для этих признаков взяты из их значений. Чтобы придать им осмысленное название, пользуйтесь параметром ```prefix=```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_dummies = pd.get_dummies(df_titanic['Age_category_1'], prefix=\"age_cat_\")\n",
    "age_dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присоединить наши новые признаки к датасету можно методом ```.join()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic = df_titanic.join(pd.get_dummies(df_titanic['Age_category_1'], prefix=\"age_cat_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Горизонтальные и вертикальные объединения, функции merge() и concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Горизонтальные\" объединения (аналог JOIN в SQL) в pandas выполняются функцией или методом ```merge()```. По умолчанию оъединение производится по колонкам с совпадающими именами и только по ключам, которые включаются в оба DataFrame'а.\n",
    "\n",
    "Создадим DataFrame с номерами грузовиков и некоторой абстрактной статистикой по ним."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks = ['X101AP', 'T123TM', 'X098AP', 'T123TM',  'X098AP', 'X101AP']\n",
    "df_trucklog = pd.DataFrame({'truck':trucks, 'week':[12,10,5,6,7,9], 'month':[212,310,85,186,217,299]}, columns=['truck', 'week', 'month'])\n",
    "df_trucklog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И создадим DataFrame со справочником по этим грузовикам, которые включают, например, марку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucks = pd.DataFrame({'plate_number': df_trucklog['truck'].unique(),\n",
    "'brand': ['VOLVO', 'RENAULT', 'MAN']}, columns=['plate_number', 'brand'])\n",
    "df_trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предположим, заказчику захотелось увидеть в отчете по этим грузовикам не только номер, но и марку, а в изначальном датасете она отсутствует. Мы можем \"вытащить\" марку из справочника, выполнив функцию ```merge()```.\n",
    "\n",
    "Укажем в параметрах названия полей, по которым надо выполнить объединение. В результате будет возвращен новый DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucklog.merge(df_trucks, left_on='truck', right_on='plate_number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в журнал по грузовикам машину, которой нет в справочнике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucklog = df_trucklog.append({'week': 5, 'month': 20, 'truck':'X055XT'}, ignore_index=True)\n",
    "df_trucklog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим, чтобы данные по этой машине также присутствовали в отчете, мы можем включить все ключи слева параметром ```how='left'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucklog.merge(df_trucks, left_on='truck', right_on='plate_number', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Вертикальное\" объединение таблиц возможно с помощью функции ```concat()```. На вход она получает список датафреймов, которые надо объединить. Если вы используете сгенерированные ключи, не забудьте указать параметр ```ignore_keys=True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucklog1 = pd.DataFrame({'truck':trucks, 'week':[2,7,6,6,2,1], 'month':[50,25,110,162,272,292]}, columns=['truck', 'week', 'month'])\n",
    "df_trucklog1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trucklog_new = pd.concat([df_trucklog, df_trucklog1], ignore_index=True)\n",
    "df_trucklog_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__???__ А как разбить DataFrame? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ЗАДАНИЕ__\n",
    "\n",
    "Есть таблица студентов и номеров их зачетных книжек. Есть несколько объектов Series с оценками по различным предметам, где индексы - номера зачетных книжек. Нужно получить следующие данные:\n",
    "1. Получить объединенный табель по всем предметам и студентам.\n",
    "2. Получить список студентов, сдавших сессию на \"хорошо\" и \"отлично\"\n",
    "3. Получить список студентов, которые сдали не все экзамены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_students = pd.DataFrame({'surname': ['Ivanov', 'Petrov', 'Sidorov', 'Kuznetsov', 'Kotova', 'Ivanov'],\\\n",
    "                           'logbook': ['X01', 'X02', 'X04', 'X03', 'X05', 'X06', ]})\n",
    "s_physics = pd.Series([5,5,2,3,4], index=['X05', 'X02', 'X03', 'X06', 'X01', ])\n",
    "s_calculus = pd.Series([4,3,5,5,4,5], index=['X02', 'X01', 'X04', 'X05', 'X06', 'X03'])\n",
    "s_linalg = pd.Series([5,2,3,4], index=['X01', 'X03', 'X05', 'X06'])\n",
    "\n",
    "# ваш код здесь\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
